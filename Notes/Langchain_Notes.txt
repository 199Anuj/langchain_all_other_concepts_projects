1. ğŸŒŸ What is LangChain?

LangChain is a Python/JavaScript framework used to build applications powered by Large Language Models (LLMs) like GPT, Llama, Groq, etc.

It helps you easily connect:

Prompts

LLM models

Tools

Memory

Chains

Agents

External APIs

Vector databases

Structured output

Retrieval systems

So instead of writing everything manually, LangChain gives ready-made building blocks.

ğŸ§© Why Do We Need LangChain?

LLMs alone only generate text.
LangChain helps LLMs:

âœ” talk to external tools
âœ” store and use memory
âœ” split tasks into steps
âœ” connect multiple prompts
âœ” output structured JSON/Pydantic
âœ” process long documents
âœ” run conditional logic
âœ” perform RAG (Retrieval Augmented Generation)
âœ” act like agents with tools

Basically, LangChain helps you build real AI applications.


2. ğŸ§± Major Components of LangChain

Below are the core building blocks every developer should know.

1ï¸âƒ£ Prompt Templates

Used for building dynamic prompts with variables.

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    template="Write a summary about {topic}",
    input_variables=["topic"]
)

2ï¸âƒ£ LLMs (Model Wrappers)

LangChain can call models from:

OpenAI

Groq

Anthropic

HuggingFace

Google

Local models

Example: from langchain_groq import ChatGroq

model = ChatGroq(model="llama-3.3-70b-versatile")


3ï¸âƒ£ Output Parsers

Used to convert raw text into:

string

JSON

Pydantic model

structured output

Example:

String Output


from langchain_core.output_parsers import StrOutputParser
parser = StrOutputParser()


4ï¸âƒ£ RunnableChains (Chains)

Chain means connecting components together:chain = prompt | model | parser
result = chain.invoke({"topic": "AI"})

Chains can be:

Sequential (step by step)

Parallel (multiple steps at once)

Conditional (if/else)


5ï¸âƒ£ Agents

Agents let LLMs:

ğŸ‘‰ decide actions
ğŸ‘‰ pick tools
ğŸ‘‰ execute tasks
ğŸ‘‰ loop until they find the answer

For example:

search Google

call APIs

do math

read files

run Python tools


6ï¸âƒ£ Memory

Stores previous conversation so LLM behaves like chat.

Types:

Buffer Memory

Summary Memory

Entity Memory

Conversation Token Buffer

Makes chatbot feel "human-like".


7ï¸âƒ£ Tools

Functions that the model can use, like:

calculator

search

code execution

custom python functions

8ï¸âƒ£ Retrievers (RAG)

Retrievers fetch relevant documents from vector DBs:

Pinecone

Chroma

FAISS

Weaviate

PGVector

Used heavily in RAG apps.

9ï¸âƒ£ Document Loaders & Text Splitters

For reading PDFs, websites, docs, etc.

--------------------------------------------------
3. âœ… What Are LangChain Models?

In LangChain, Models are the core building blocks used to interact with language models (LLMs) or other AI models.
LangChain provides a unified interface so you can plug in OpenAI, HuggingFace, Gemini, Groq, Ollama, or any other model without changing your code.

LangChain mainly has three types of models:

1ï¸âƒ£ LLMs (Large Language Models) â€“ Text Completion Models

These models take a string input and return a string output.
Example providers:

OpenAI GPT

Groq Llama

HuggingFace models

Ollama


2ï¸âƒ£ ChatModels â€“ Structured Chat Message Models

These models work with messages instead of plain text.

LangChain defines message types like:

SystemMessage

HumanMessage

AIMessage

ChatModels understand multi-turn prompts, history, and roles.

Example providers:

OpenAI ChatCompletion

Groq Chat Models

Gemini Chat

Mistral Chat

Inout - [
  SystemMessage("You are a helpful assistant."),
  HumanMessage("Write a poem about stars")
]

Output - AIMessage("The stars dance softly in the night...")


3ï¸âƒ£ Embedding Models

These convert text into vectors (lists of numbers).
Used for:

Semantic search

RAG (Retrieval-Augmented Generation)

Clustering

Similarity comparisons

Example providers:

HuggingFace Embedding Models

OpenAI Embeddings

Google Embeddings

SentenceTransformers
---------------------------------------------

4. âœ… Difference Between LLMs and ChatModels in LangChain

LangChain supports two major text-generation model types:

1ï¸âƒ£ LLMs (Large Language Models)

âœ” Work with plain text strings
âœ” Input â†’ string
âœ” Output â†’ string

Example: llm.invoke("Write a poem about the moon")


LLMs do not understand chat roles (system/human/assistant).
They just generate output based on raw text.

2ï¸âƒ£ ChatModels

âœ” Work with structured messages
âœ” Each message has a role:

system

user (human)

assistant (AI)

Example: chat.invoke([
    ("system", "You are a helpful assistant."),
    ("user", "Write a poem about the moon.")
])


â­ Can ChatModels take string input like LLMs?
Yes â€” LangChain automatically converts a string into a HumanMessage.

Example: chat.invoke("Tell me a joke")

This internally becomes: [HumanMessage(content="Tell me a joke")]

0---------------------------------------------------------------------------------------

5. âœ… What Are Prompts in LangChain?

In LangChain, prompts are structured inputs given to LLMs / ChatModels to guide their behavior.

A prompt is not just text â€” LangChain turns prompts into reusable, composable Prompt Templates.

âœ” Purpose of Prompts

Control model behavior

Inject instructions

Format dynamic inputs

Reduce prompt-writing repetition

Enable consistent outputs for agents, chains, and tools


âœ… ğŸ”¹ Static Prompts vs Dynamic Prompts
1ï¸âƒ£ Static Prompts

A static prompt is fixed text only.
It does NOT take variables, does not change, and is written as plain string.

prompt = "Write a poem about artificial intelligence."

â¡ Always the same
â¡ Not reusable
â¡ No placeholders
â¡ Cannot accept user input programmatically

2ï¸âƒ£ Dynamic Prompts

A dynamic prompt changes based on variables inserted at runtime.

You define variables like {topic}, {tone}, {language}, etc.

ğŸŸ¦ Why Prompt Templates are Used in LangChain? (Very Important for Interviews)

Here are the main reasons:

âœ” 1. To Make Prompts Reusable

You don't want to rewrite the same prompt again and again.


âœ” 2. To Insert Dynamic Data Easily

Prompt Templates allow you to pass dynamic values:

âœ” user input
âœ” database results
âœ” extracted text
âœ” chain outputs

3. To Enforce Structured Prompting

Prompt Templates can include model output instructiins:

format_instructions

JSON schema

Pydantic structures

4.  To Support Complex Chains & Agents

Every chain needs prompts with placeholders:

RAG chains

Classification pipelines

Sentiment â†’ Generate reply pipelines

Agents using tools

PromptTemplate makes it easy to pass outputs between chains.

Example -> from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    template="Write a short poem about {topic}.",
    input_variables=["topic"]
)

formatted_prompt = prompt.format(topic="artificial intelligence")

print(formatted_prompt)

--------------------------------------------

6. ğŸŸ¦ What Is Structured Output in LangChain?

Structured Output means telling the LLM to return results in a strict, machine-readable format such as:

JSON

Pydantic model

Dictionary

Typed schema

This is used when you want guaranteed structure such as:
âœ” classification
âœ” extraction
âœ” sentiment analysis
âœ” structured summaries
âœ” database filling
âœ” agent tool outputs

ğŸŸ¦ Two Ways LangChain Supports Structured Output
âœ… 1. Using Model-Native Structured Output (RECOMMENDED)

Some LLMs can generate structured JSON natively.

These models can follow structured formats very reliably because the model itself supports JSON mode or tool calling.

Examples of models that support structured output:
Model	Support Type
OpenAI GPT-4o, GPT-4.1, GPT-3.5 Turbo	JSON Mode + Function Calling
Groq LLaMA 3.1 / 3.3 models	JSON Structured Output
Anthropic Claude 3 models	Tool Calling + JSON
Gemini Flash / Pro	Function calling
Mistral models	JSON support varies
âœ… 2. Using LangChain Output Parsers (Works with ANY model)

Even if the model does not support JSON mode natively, LangChain output parsers can enforce structure.

ğŸŸ¦ Main Output Parsers in LangChain

Here are the most important LangChain output parsers:

â­ 1. StrOutputParser

âœ” Returns raw text
âœ” Used when you donâ€™t need structure

Returns the Output in string format


â­ 2. PydanticOutputParser (Most Important)

âœ” Converts model output into a Pydantic class
âœ” Automatically generates format instructions

we va inforce schema and even validation

â­ 3. JsonOutputParser

âœ” Ensures JSON only
âœ” Useful when you want plain schemas

inforces json scehma to output of the model. we have to provide schema as well which we wnat in the output.

But we cannot impose any validation in the output. fore xample sentiment analysis. tehe value should be ositive or negative

-------------------------------------------------------------------------------
7. ğŸš€ What Are Chains in LangChain?

A Chain in LangChain is a pipeline that connects multiple steps together, such as:

Taking input

Formatting prompts

Calling LLMs/chat models

Parsing output

Using tools

Running conditional logic

Combining results

Think of a chain like a workflow, where each block does something and passes output to the next.

There are three types of chain -:

A) Sequencial chains.
Executes steps one after another. 
EG -> chain = prompt | model | parser

B) Parallel Chains.
Runs different chains based on a condition.
branch = RunnableBranch(
    (lambda x: len(x.split()) > 200, summary_chain),
    RunnablePassthrough()
)

C) Conditional Chains
Runs multiple chains at the same time and returns combined output.
chain = RunnableParallel({
    'summary'=summary_chain,
    'sentiment'=sentiment_chain
})
------------------------------------------------------------
8. Runnables ->
In LangChain, Runnables are the core building blocks used to create pipelines, chains, and workflows in the new LangChain Expression Language (LCEL).
They are flexible, composable units that represent anything that can be executed â€” a model call, a prompt, a function, a branching operation, or an entire chain.

âœ… What Are Runnables?

A Runnable is simply something you can â€œrunâ€ (execute) with .invoke(), .batch(), or .stream().

Examples of runnables:

Prompt Templates

LLMs / ChatModels

Output Parsers

Functions (via RunnableLambda)

Branching

Sequences / pipelines

Maps

Parallel execution

Retrievers

Tool Calls

â­ Why Runnables Are Important

Before LCEL, LangChain had many different chain types.

Now:

Everything is a Runnable.
we dont need to remember different chains but just create runnables accodinging to our requirement which creates chains and then we have to call invoke fucntion.

ğŸ§© Types of Runnables (Key Runnables in LangChain)

Below are the most commonly used Runnables.

1ï¸âƒ£ RunnableLambda

Used to wrap any Python function as a runnable.

from langchain_core.runnables import RunnableLambda

word_count = RunnableLambda(lambda x: len(x.split()))

2ï¸âƒ£ RunnableSequence

A chain of multiple runnables (serial execution), using |.

pipeline = prompt | model | parser

This is just syntactic sugar for a sequence.

3ï¸âƒ£ RunnableBranch

Implements ifâ€“else logic.

from langchain_core.runnables import RunnableBranch

branch_chain = RunnableBranch(
    (lambda x: len(x.split()) > 200, long_prompt | model | parser),
    default=short_prompt | model | parser
)

4ï¸âƒ£ RunnableParallel (RunnableMap)

Runs multiple runnables in parallel.

from langchain_core.runnables import RunnableParallel

parallel = RunnableParallel(
    sentiment = model1,
    summary = model2
)

5ï¸âƒ£ RunnablePassthrough

Passes input unchanged.

Useful in branches.

from langchain_core.runnables import RunnablePassthrough


--------------------------------------------------------------------
9. Document Loaders in LangChain are utilities that load data from different sources (files, URLs, databases, APIs, etc.) and convert them into Document objects that LLMs can process.

They are the first step in most LangChain pipelines (RAG, QA, summarization, indexing).

âœ… What are Document Loaders?

A Document Loader:

Reads data from a source

Extracts the content

Returns a list of Document objects

Each Document contains:

page_content â†’ the actual text

metadata â†’ filename, URL, page number, etc.

ğŸ“Œ Most Important Document Loaders in LangChain

Here are the most commonly used & practical loaders:

ğŸ”¹ 1. TextLoader

Loads text files (.txt).

ğŸ”¹ 2. CSVLoader

Loads CSV files row-by-row.

ğŸ”¹ 3. PDF Loaders

Multiple options exist:

â†’ PyPDFLoader

Best for simple PDFs.


ğŸ”¹ 4. WebBaseLoader

Loads & scrapes webpage content.

ğŸ”¹ 5. DirectoryLoader

Loads ALL files from a folder.
------------------------------------------------------------------------------
10. Text splitters are a core concept in LangChain and are used in almost every RAG pipeline, summarization pipeline, and LLM-based document processing workflow.

Here is the complete, simple, clear explanation ğŸ‘‡

âœ… What Are Text Splitters?

Text Splitters are utilities that break large text/documents into smaller chunks.

LLMs (GPT, Llama, Gemini, Claude) have context limits, so you cannot pass very large documents directly.
Text splitters divide documents into meaningful segments so they can be:

indexed in vector databases

searched using embeddings

summarized efficiently

fed to LLMs chunk by chunk

ğŸ¯ Why Are Text Splitters Used? (Purpose)

Text splitters help with:

1ï¸âƒ£ Avoiding Token Limit Errors

LLMs cannot handle very long text, so splitting is required.

2ï¸âƒ£ Better Embedding Quality

Small, clean chunks produce high-quality embeddings â†’ better search accuracy.

3ï¸âƒ£ Maintaining Context

Good splitters ensure chunks do not break mid-sentence or mid-paragraph.

4ï¸âƒ£ Efficient Retrieval in RAG

Chunks act like "knowledge units" that the retriever uses.

5ï¸âƒ£ Faster Processing

Working with small chunks improves speed/cost.

â­ Types of Text Splitters in LangChain

Below is the complete list of important text splitters and when to use them.

ğŸ“Œ 1. Character-based Splitters

Split text based on characters.

ğŸ“Œ 2. RecursiveCharacterTextSplitter (Most Recommended)

The best general-purpose splitter.

It tries splitting by:

paragraphs

sentences

words

characters

Keeps natural boundaries.

3. Recursive text splitters based on languafe like python markup language.

It splits based on calss fucntions then paragraph words etc

4. ğŸ“Œ 7. Semantic Splitters (Meaning-based)

(Advanced, AI-powered)

Names:

SemanticTextSplitter

SemanticChunker (newer versions)

These use embeddings to split text by meaning.
----------------------------------------------------------------------
11. ğŸ§  What are Vector Stores? (Plain English)

A vector store is a special database that:

Stores numbers (vectors) that represent the meaning of text, images, or other data
and lets you find the most similar ones quickly.

Instead of searching by keywords, it searches by meaning.